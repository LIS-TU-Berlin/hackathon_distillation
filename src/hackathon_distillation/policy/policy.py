#!/usr/bin/env python

# Copyright 2024 Columbia Artificial Intelligence, Robotics Lab,
# and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Diffusion Policy as per "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"""

from collections import deque
from pathlib import Path

import hydra.utils
import numpy as np
import torch as th
from omegaconf import OmegaConf
from torch import nn, Tensor


def populate_queues(queues, batch):
    for key in batch:
        if key not in queues:
            continue
        if len(queues[key]) != queues[key].maxlen:
            while len(queues[key]) != queues[key].maxlen:
                queues[key].append(batch[key])
        else:
            queues[key].append(batch[key])
    return queues


class Policy(nn.Module):
    """
    Reactive Policy as per "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
    (paper: https://arxiv.org/abs/2303.04137, code: https://github.com/real-stanford/diffusion_policy).
    """

    name = "policy"

    def __init__(
        self,
        checkpoint_path: Path,
        map_location: str | th.device = "cpu",
    ):
        """
        Args:
            config: Policy configuration class instance or None, in which case the default instantiation of
                the configuration class is used.
            dataset_stats: Dataset statistics to be used for normalization. If not passed here, it is expected
                that they will be passed with a call to `load_state_dict` before the policy is used.
        """
        super().__init__()

        # queues are populated during rollout of the policy, they contain the n latest observations and actions
        self._queues = None

        cfg = OmegaConf.load(f"{checkpoint_path.parent}/config.yaml")
        self.policy = hydra.utils.get_class(cfg.model_type).from_checkpoint(checkpoint_path, map_location)
        self._model = self.policy.model
        self._model.to(map_location)

        self.expected_image_keys = [k for k in self.policy.config.network.input_shapes if k.startswith("observation.image")]
        self.use_env_state = "observation.state" in self.policy.config.network.input_shapes

        self.reset()

    @property
    def config(self):
        return self.policy.config

    def reset(self):
        """Clear observation and action queues. Should be called on `env.reset()`"""
        self._queues = {
            "action": deque(maxlen=self.config.action_horizon),
        }
        if len(self.expected_image_keys) > 0:
            self._queues["observation.image"] = deque(maxlen=self.config.obs_horizon)
        if self.use_env_state:
            self._queues["observation.state"] = deque(maxlen=self.config.obs_horizon)
        self.policy.prior_sample = None

    @th.no_grad()
    def select_action(self, batch: dict[str, Tensor]) -> np.ndarray:
        """Select a single action given environment observations.

        This method handles caching a history of observations and an action trajectory generated by the
        underlying diffusion model. Here's how it works:
          - `n_obs_steps` steps worth of observations are cached (for the first steps, the observation is
            copied `n_obs_steps` times to fill the cache).
          - The diffusion model generates `horizon` steps worth of actions.
          - `n_action_steps` worth of actions are actually kept for execution, starting from the current step.
        Schematically this looks like:
            ----------------------------------------------------------------------------------------------
            (legend: o = n_obs_steps, h = horizon, a = n_action_steps)
            |timestep            | n-o+1 | n-o+2 | ..... | n     | ..... | n+a-1 | n+a   | ..... | n-o+h |
            |observation is used | YES   | YES   | YES   | YES   | NO    | NO    | NO    | NO    | NO    |
            |action is generated | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   |
            |action is used      | NO    | NO    | NO    | YES   | YES   | YES   | NO    | NO    | NO    |
            ----------------------------------------------------------------------------------------------
        Note that this means we require: `n_action_steps <= horizon - n_obs_steps + 1`. Also, note that
        "horizon" may not the best name to describe what the variable actually means, because this period is
        actually measured from the first observation which (if `n_obs_steps` > 1) happened in the past.
        """
        batch = self.policy.normalize_inputs(batch)  # works
        if len(self.expected_image_keys) > 0:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch["observation.image"] = th.stack([batch[k] for k in self.expected_image_keys], dim=-4)
        # Note: It's important that this happens after stacking the images into a single key.
        self._queues = populate_queues(self._queues, batch)

        if len(self._queues["action"]) == 0:
            # stack n latest observations from the queue
            batch = {k: th.stack(list(self._queues[k]), dim=1) for k in batch if k in self._queues}
            actions = self.policy.generate_actions(batch)
            #actions = self.policy.unnormalize_outputs({"action": actions})["action"]

            self._queues["action"].extend(actions.transpose(0, 1))

        action = self._queues["action"].popleft()
        return action.detach()

    def forward(self, batch: dict[str, Tensor]) -> dict[str, Tensor]:
        """Run the batch through the model and compute the loss.

        This is really a placeholder.
        """
        out = self.policy.compute_loss(self._model, batch)
        return out
