defaults:
  - image_transforms: default
  - _self_

# Data
log_dir: "logs"
ckpt_save_path: "ckpts"
misc_save_path: "misc"
is_debug: true
#run_name: "ddpm_unet_rgb"

# Training hyperparams
epochs: 200
batch_size: 32   # this is per device if ddp is used
grad_clip_norm: 10.
seed: 123
port: 12335
devices: [0, 1]
strategy: ddp  # [cuda, ddp, fsdp]; cuda is single gpu, ddp is data multigpu, fsdp is model & data different gpus
val_every: 10
save_every: 20
save_max: 3

# Prediction setup
#n_action_steps: 8  # execution horizon
#n_obs_steps: 1    # Use 0 for unconditional generation -- not used during training
#horizon: 128  # prediction horizon -- what the actual model predicts
obs_horizon: 2
pred_horizon: 4
action_horizon: 2

# Policy options
model_type: src.hackathon_distillation.policy.ddpm_wrapper.DdpmWrapper
num_inference_steps: 10   # How many steps are used for denoising during inference; if null this is eq. to num of train steps
do_mask_loss_for_padding: false
drop_n_last_frames: True
prediction_type: "epsilon"

# Network
network:
  dropout: 0.
  down_dims: [ 512, 1024, 2048 ]
  input_shapes: {
    "obs.depth": [1, 360, 640],  # comment out for no images
#    "obs.state": [ 3 ],
  }
  input_normalization_modes: {
    "obs.depth": "mean_std",
    #"obs.state": "min_max",
  }
  output_shapes: {
    "action": [ 3 ],  # Note we need feature dim first! Find out why and how this works; feature dim is ndof
  }
  output_normalization_modes: {
    "action": "min_max",
  }
  pretrained_backbone_weights: null #'DEFAULT'    # comment out for no pretrained backbone
  use_group_norm: False
  n_groups: 8
  vision_backbone: resnet18
  crop_shape: [ 84, 84 ]
  crop_is_random: True
  spatial_softmax_num_keypoints: 256

  # Unet.
  network_cls: hackathon_distillation.networks.unet.ConditionalUnet1d
  kernel_size: 5
  diffusion_step_embed_dim: 128
  use_film_scale_modulation: True

noise_scheduler:
  _target_: diffusers.schedulers.scheduling_ddpm.DDPMScheduler

  num_train_timesteps: 10
  beta_schedule: squaredcos_cap_v2
  beta_start: 0.0001
  beta_end: 0.02
  prediction_type: epsilon # epsilon / sample
  clip_sample: True
  clip_sample_range: 1.0

optimizer:
  _target_: torch.optim.AdamW
  kwargs:
    lr: 5e-4  # defaults to 0.0025
    # betas: [ 0.95, 0.999 ]
    eps: 1e-8
    weight_decay: 1e-6
    # warmup_steps: 500
